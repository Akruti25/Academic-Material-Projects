# -*- coding: utf-8 -*-
"""asinha6_HW3_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VjBBs7AJRmWyI8Tbr_jbamExvwhZZL5y
"""

#Importing stuff
#!pip install tld
import argparse
import json
from urllib.parse import urlparse
from tld import get_fld

#Reading har files
def read_harfile(harfile_path):
  harfile = open(harfile_path, encoding="utf-8")
  harfile_json = json.loads(harfile.read())
  i = 0
  third_party_domains = set()
  for entry in harfile_json['log']['entries']:
    i = i + 1
    url = entry['request']['url']

    effective_top_level_domain = get_fld(url)
    if effective_top_level_domain != 'macys.com' and effective_top_level_domain != 'cnn.com':
      third_party_domains.add(effective_top_level_domain)

  return third_party_domains

# Count the number of unique third-party domains
def count_unique_third_party_domains(third_party_domains):
  unique_third_party_domains = set()
  for domain in third_party_domains:
    unique_third_party_domains.add(domain)
  return len(unique_third_party_domains)

# Print the number of unique third-party domains for each website
print("Answer to part a")
macys_third_party_domains = read_harfile('www.macys.com.har')
cnn_third_party_domains = read_harfile('www.cnn.com.har')

print("The number of unique third-party domains loaded while visiting macys.com is", count_unique_third_party_domains(macys_third_party_domains))
print("The number of unique third-party domains loaded while visiting cnn.com is", count_unique_third_party_domains(cnn_third_party_domains))

print("Answer to part b:")
# Find the unique third-party domains that appear on both macys.com and cnn.com
common_third_party_domains = macys_third_party_domains.intersection(cnn_third_party_domains)

# Print the common third-party domains
print("The unique third-party domains that appear on both macys.com and cnn.com are:", common_third_party_domains)
print("The number of the common domains are:", len(common_third_party_domains))

import json
from tld import get_fld

with open('disconnect.json', 'r', encoding='utf-8') as disconnect_file:
    disconnect_data = json.load(disconnect_file)
    disconnect_domains = set()

    for category_entries in disconnect_data['categories'].values():
        for category_dict in category_entries:
            for domain_info_dict in category_dict.values():
                for domain_key, domain_values_list in domain_info_dict.items():
                    for domain_value in domain_values_list:
                        key_domain = get_fld(domain_key, fail_silently=True)
                        if key_domain:
                            disconnect_domains.add(key_domain)
                            disconnect_domains.add(domain_value)
#print(len(disconnect_domains))

def blocked_requests(harfile_path, disconnect_domains):
  harfile = open(harfile_path, encoding='utf-8')
  harfile_json = json.load(harfile)
  blocked_requests = 0

  for entry in harfile_json['log']['entries']:
    url = entry['request']['url']
    domain = get_fld(url, fail_silently=True)

    if domain in disconnect_domains:
      blocked_requests += 1

  return blocked_requests

print("Answer to part c:")

blocked_macys_requests = blocked_requests("www.macys.com.har", disconnect_domains)
blocked_cnn_requests = blocked_requests("www.cnn.com.har", disconnect_domains)
# Print the number of blocked requests
print('Website | # of requests blocked')
print('------- | --------')
print('Macys   |', blocked_macys_requests)
print('CNN     |', blocked_cnn_requests)