# -*- coding: utf-8 -*-
"""asinha6_HW5_Q2c.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OPW0431gXSK9NgLgOEh0AiMBhwF6I4hN
"""

#!pip install nltk

#!pip install requests

"""APP 1: BUMBLE"""

import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
import collections

def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name())
    return synonyms

def create_bag_of_words(keywords):
    bag_of_words = set()
    for word in keywords:
        bag_of_words.add(word)
        synonyms = get_synonyms(word)
        bag_of_words.update(synonyms)
    return bag_of_words

def load_policy(file_path):
    with open(file_path, 'r', encoding="utf8") as f:
        return [sentence.split() for sentence in f.read().splitlines()]

def annotate_policy(policy, sets_checkpoints, threshold):
    print()

    for idx, bag_of_words in enumerate(sets_checkpoints):
        # Collecting matching lines
        matching_lines = []
        for sentence in policy:
            if len(bag_of_words.intersection(set(sentence))) > threshold:
                matching_lines.append(' '.join(sentence))

        # Print matching lines
        if matching_lines:
            print('------------------------------------------')
            print('Here are the sentences that match the checkpoint:')
            for line in matching_lines:
                print(line)
            print()
            print('-----------------------------------')
            print('Checkpoint:')
            print(checkpoints[idx])
            print()

checkpoints = [
    "Data collection and purpose",
    "Data sharing",
    "User Control",
    "User Right",
    "Data Retention",
    "Data protection",
    "Policy Change Notification",
    "Restriction for specific audience"
]

# list of keywords from each checkpoint
bag_of_words = [
    {'collects', 'user information', 'purpose', 'how', 'why'},
    {'user information', 'shared', 'collected', 'third parties', 'share', 'sharing'},
    {'choices', 'control options', 'user', 'control', 'options'},
    {'users', 'access', 'edit', 'delete', 'information'},
    {'mentions', 'long', 'user information', 'stored'},
    {'mentions', 'user information', 'protected', 'protect'},
    {'users', 'informed', 'changes', 'privacy policy', 'notification'},
    {'mentions', 'practices', 'specific group', 'children', 'Europeans', 'California residents'}
]

sets_checkpoints = [create_bag_of_words(keywords) for keywords in bag_of_words]

# Load the policy
policy = load_policy('policy1.txt')

# Annotate policy for each checkpoint
for checkpoint_index in range(len(checkpoints)):
    print(f"\nCheckpoint {checkpoint_index + 1}: {checkpoints[checkpoint_index]}\n")
    print('-----------------------------------')
    annotate_policy(policy, sets_checkpoints, 1)

"""APP 2: DUOLINGO:"""

import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
import collections

def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name())
    return synonyms

def create_bag_of_words(keywords):
    bag_of_words = set()
    for word in keywords:
        bag_of_words.add(word)
        synonyms = get_synonyms(word)
        bag_of_words.update(synonyms)
    return bag_of_words

def load_policy(file_path):
    with open(file_path, 'r', encoding="utf8") as f:
        return [sentence.split() for sentence in f.read().splitlines()]

def annotate_policy(policy, sets_checkpoints, threshold):
    print()

    for idx, bag_of_words in enumerate(sets_checkpoints):
        # Collecting matching lines
        matching_lines = []
        for sentence in policy:
            if len(bag_of_words.intersection(set(sentence))) > threshold:
                matching_lines.append(' '.join(sentence))

        # Print matching lines
        if matching_lines:
            print('------------------------------------------')
            print('Here are the sentences that match the checkpoint:')
            for line in matching_lines:
                print(line)
            print()
            print('-----------------------------------')
            print('Checkpoint:')
            print(checkpoints[idx])
            print()

checkpoints = [
    "Data collection and purpose",
    "Data sharing",
    "User Control",
    "User Right",
    "Data Retention",
    "Data protection",
    "Policy Change Notification",
    "Restriction for specific audience"
]

# list of keywords from each checkpoint
bag_of_words = [
    {'collects', 'user information', 'purpose', 'how', 'why'},
    {'user information', 'shared', 'collected', 'third parties', 'share', 'sharing'},
    {'choices', 'control options', 'user', 'control', 'options'},
    {'users', 'access', 'edit', 'delete', 'information'},
    {'mentions', 'long', 'user information', 'stored'},
    {'mentions', 'user information', 'protected', 'protect'},
    {'users', 'informed', 'changes', 'privacy policy', 'notification'},
    {'mentions', 'practices', 'specific group', 'children', 'Europeans', 'California residents'}
]

sets_checkpoints = [create_bag_of_words(keywords) for keywords in bag_of_words]

# Load the policy
policy = load_policy('policy2.txt')

# Annotate policy for each checkpoint
for checkpoint_index in range(len(checkpoints)):
    print(f"\nCheckpoint {checkpoint_index + 1}: {checkpoints[checkpoint_index]}\n")
    print('-----------------------------------')
    annotate_policy(policy, sets_checkpoints, 1)

"""APP 3: CamScanner"""

import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
import collections

def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name())
    return synonyms

def create_bag_of_words(keywords):
    bag_of_words = set()
    for word in keywords:
        bag_of_words.add(word)
        synonyms = get_synonyms(word)
        bag_of_words.update(synonyms)
    return bag_of_words

def load_policy(file_path):
    with open(file_path, 'r', encoding="utf8") as f:
        return [sentence.split() for sentence in f.read().splitlines()]

def annotate_policy(policy, sets_checkpoints, threshold):
    print()

    for idx, bag_of_words in enumerate(sets_checkpoints):
        # Collecting matching lines
        matching_lines = []
        for sentence in policy:
            if len(bag_of_words.intersection(set(sentence))) > threshold:
                matching_lines.append(' '.join(sentence))

        # Print matching lines
        if matching_lines:
            print('------------------------------------------')
            print('Here are the sentences that match the checkpoint:')
            for line in matching_lines:
                print(line)
            print()
            print('-----------------------------------')
            print('Checkpoint:')
            print(checkpoints[idx])
            print()

checkpoints = [
    "Data collection and purpose",
    "Data sharing",
    "User Control",
    "User Right",
    "Data Retention",
    "Data protection",
    "Policy Change Notification",
    "Restriction for specific audience"
]

# list of keywords from each checkpoint
bag_of_words = [
    {'collects', 'user information', 'purpose', 'how', 'why'},
    {'user information', 'shared', 'collected', 'third parties', 'share', 'sharing'},
    {'choices', 'control options', 'user', 'control', 'options'},
    {'users', 'access', 'edit', 'delete', 'information'},
    {'mentions', 'long', 'user information', 'stored'},
    {'mentions', 'user information', 'protected', 'protect'},
    {'users', 'informed', 'changes', 'privacy policy', 'notification'},
    {'mentions', 'practices', 'specific group', 'children', 'Europeans', 'California residents'}
]

sets_checkpoints = [create_bag_of_words(keywords) for keywords in bag_of_words]

# Load the policy
policy = load_policy('policy3.txt')

# Annotate policy for each checkpoint
for checkpoint_index in range(len(checkpoints)):
    print(f"\nCheckpoint {checkpoint_index + 1}: {checkpoints[checkpoint_index]}\n")
    print('-----------------------------------')
    annotate_policy(policy, sets_checkpoints, 1)